# -*- coding: utf-8 -*-
"""POTHOLE_SEGMENTATION.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ugQPwZUE5NxNidFYGQHEIeZyX2k2GHdq
"""

!pip install ultralytics

from google.colab import drive
drive.mount('/content/drive')

import warnings
warnings.filterwarnings('ignore')
import os
import shutil
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import random
import cv2
import yaml
from PIL import Image
from collections import deque
from ultralytics import YOLO
from IPython.display import Video

model = YOLO('yolov8n-seg.pt')

dataset_path='/content/drive/MyDrive/Pothole_Segmentation_YOLOv8'
yaml_file_path = os.path.join(dataset_path, 'data.yaml')

train_images_path = os.path.join(dataset_path, 'train', 'images')
valid_images_path = os.path.join(dataset_path, 'valid', 'images')

num_train_images = 0
num_valid_images = 0
train_image_sizes = set()
valid_image_sizes = set()

for filename in os.listdir(train_images_path):
    if filename.endswith('.jpg'):
        num_train_images += 1
        image_path = os.path.join(train_images_path, filename)
        with Image.open(image_path) as img:
            train_image_sizes.add(img.size)
for filename in os.listdir(valid_images_path):
    if filename.endswith('.jpg'):
        num_valid_images += 1
        image_path = os.path.join(valid_images_path, filename)
        with Image.open(image_path) as img:
            valid_image_sizes.add(img.size)
print(f"Number of training images: {num_train_images}")
print(f"Number of validation images: {num_valid_images}")

if len(train_image_sizes) == 1:
    print(f"All training images have the same size: {train_image_sizes.pop()}")
else:
    print("Training images have varying sizes.")

if len(valid_image_sizes) == 1:
    print(f"All validation images have the same size: {valid_image_sizes.pop()}")
else:
    print("Validation images have varying sizes.")

"""Fine-tune the model on our dataset of pothole images for segmentation"""

results = model.train(
    data=yaml_file_path,
    epochs=150,
    imgsz=640,
    patience=15,
    batch=16,
    optimizer='auto',
    lr0=0.0001,
    lrf=0.01,
    dropout=0.25,
    device=0,
    seed=42
)

best_model = YOLO('runs/segment/train/weights/best.pt')

valid_images_path = os.path.join(dataset_path, 'valid', 'images')

image_files = [file for file in os.listdir(valid_images_path) if file.endswith('.jpg')]

num_images = len(image_files)
selected_images = [image_files[i] for i in range(0, num_images, num_images // 9)]

fig, axes = plt.subplots(3, 3, figsize=(20, 21))
fig.suptitle('Validation Set Inferences', fontsize=24)

for i, ax in enumerate(axes.flatten()):
    image_path = os.path.join(valid_images_path, selected_images[i])
    results = best_model.predict(source=image_path, imgsz=640)
    annotated_image = results[0].plot()
    annotated_image_rgb = cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB)
    ax.imshow(annotated_image_rgb)
    ax.axis('off')

plt.tight_layout()
plt.show()

dataset_video_path = '/content/drive/MyDrive/Pothole_Segmentation_YOLOv8/sample_video.mp4'

video_path = '/content/sample_video.mp4'
shutil.copyfile(dataset_video_path, video_path)
best_model.predict(source=video_path, save=True)

input_video = '/content/drive/MyDrive/Pothole_Segmentation_YOLOv8/sample_video.mp4'
working_dir = '/content/'
output_avi = os.path.join(working_dir, 'runs/segment/predict/sample_video.avi')
output_mp4 = os.path.join(working_dir, 'processed_sample_video.mp4')
!ffmpeg -y -loglevel panic -i {output_avi} {output_mp4}
Video("processed_sample_video.mp4", embed=True, width=960)

best_model.export(format='onnx')

valid_images_dir = os.path.join(dataset_path, 'valid', 'images')
all_images = [img for img in os.listdir(valid_images_dir) if img.lower().endswith('.jpg')]
chosen_image = all_images[30]

full_image_path = os.path.join(valid_images_dir, chosen_image)
predictions = best_model.predict(source=full_image_path, imgsz=640, conf=0.5)
result = predictions[0]

annotated_img = result.plot()
annotated_img_rgb = cv2.cvtColor(annotated_img, cv2.COLOR_BGR2RGB)


total_damage_pixels = 0
total_pixels = 0


num_plots = 1
if result.masks is not None:
    num_plots += len(result.masks.data)


plt.figure(figsize=(15, 5))

plt.subplot(1, num_plots, 1)
plt.imshow(annotated_img_rgb)
plt.title('Original Image')
plt.axis('off')


if result.masks is not None:
    masks = result.masks.data.cpu().numpy()

    for i, mask in enumerate(masks):
        binary_mask = (mask > 0).astype(np.uint8) * 255


        damage_pixels = np.sum(binary_mask == 255)
        total_damage_pixels += damage_pixels


        if i == 0:
            total_pixels = binary_mask.size

        damage_percentage = (damage_pixels / total_pixels) * 100

        print(f"Mask {i+1}:")
        print(f"- Damage pixels: {damage_pixels}")
        print(f"- Local damage %: {damage_percentage:.2f}%")
        print("-" * 30)

        plt.subplot(1, num_plots, i+2)
        plt.imshow(binary_mask, cmap='gray')
        plt.title(f'Damage Zone {i+1}\n{damage_percentage:.2f}%')
        plt.axis('off')
    total_damage_percentage = (total_damage_pixels / total_pixels) * 100

    print("\n=== TOTAL ROAD DAMAGE ===")
    print(f"- Total damage pixels: {total_damage_pixels}")
    print(f"- Total road damage %: {total_damage_percentage:.2f}%")

    plt.suptitle(f"Total Road Damage: {total_damage_percentage:.2f}%", y=0.95, fontsize=12)

plt.tight_layout()
plt.show()

video_path = '/content/drive/MyDrive/Pothole_Segmentation_YOLOv8/sample_video.mp4'


font = cv2.FONT_HERSHEY_SIMPLEX
font_scale = 1
text_position = (40, 80)
font_color = (255, 255, 255)
background_color = (0, 0, 255)

damage_deque = deque(maxlen=10)

cap = cv2.VideoCapture(video_path)

fourcc = cv2.VideoWriter_fourcc(*'XVID')
out = cv2.VideoWriter('road_damage_assessment.avi', fourcc, 20.0, (int(cap.get(3)), int(cap.get(4))))

while cap.isOpened():
    ret, frame = cap.read()
    if ret:
        results = best_model.predict(source=frame, imgsz=640, conf=0.25)
        processed_frame = results[0].plot(boxes=False)
        percentage_damage = 0


        if results[0].masks is not None:
            total_area = 0
            masks = results[0].masks.data.cpu().numpy()
            image_area = frame.shape[0] * frame.shape[1]
            for mask in masks:
                binary_mask = (mask > 0).astype(np.uint8) * 255
                contour, _ = cv2.findContours(binary_mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
                total_area += cv2.contourArea(contour[0])

            percentage_damage = (total_area / image_area) * 100


        damage_deque.append(percentage_damage)
        smoothed_percentage_damage = sum(damage_deque) / len(damage_deque)

        cv2.line(processed_frame, (text_position[0], text_position[1] - 10),
                 (text_position[0] + 350, text_position[1] - 10), background_color, 40)

        cv2.putText(processed_frame, f'Road Damage: {smoothed_percentage_damage:.2f}%', text_position, font, font_scale, font_color, 2, cv2.LINE_AA)


        out.write(processed_frame)
        # cv2.imshow('Road Damage Assessment', processed_frame)
        # if cv2.waitKey(1) & 0xFF == ord('q'):
        #     break
    else:
        break


cap.release()
out.release()

# cv2.destroyAllWindows()

!ffmpeg -y -loglevel panic -i road_damage_assessment.avi road_damage_assessment.mp4

Video("road_damage_assessment.mp4", embed=True, width=960)

THINGSPEAK_CHANNEL_ID = "YOUR_CHANNEL_ID"
THINGSPEAK_WRITE_API_KEY = "YOUR_WRITE_API_KEY"


if frame_count % 10 == 0:
    send_to_thingspeak(smoothed_percentage_damage, THINGSPEAK_CHANNEL_ID, THINGSPEAK_WRITE_API_KEY)
frame_count += 1

import requests
import time
import geocoder

def send_to_thingspeak(damage_percentage, channel_id, write_api_key):

    g = geocoder.ip('me')
    lat, lng = g.latlng if g.latlng else (0, 0)

    url = f"https://api.thingspeak.com/update?api_key={write_api_key}"
    payload = {
        'field1': damage_percentage,
        'field2': lat,
        'field3': lng
    }

    try:
        response = requests.get(url, params=payload)
        print(f"Data sent to ThingSpeak. Response: {response.status_code}")
    except Exception as e:
        print(f"Error sending to ThingSpeak: {e}")